{"nbformat": 4, "nbformat_minor": 5, "metadata": {"colab": {"name": "sparse_index_tracking_starter.ipynb", "provenance": [], "collapsed_sections": []}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}}, "cells": [{"cell_type": "markdown", "metadata": {"id": "title"}, "source": ["# Sparse Index Tracking \u2014 Starter Notebook\n", "\n", "_Generated 2025-09-02_\n", "\n", "This notebook provides a **clean, modular baseline** for building a sparse portfolio that tracks a benchmark (default **S&P 500, ^GSPC**). It includes:\n", "- Data ingestion (tickers + prices via `pandas`/`yfinance`)\n", "- Benchmark returns (^GSPC)\n", "- Two methods:\n", "  1. **Greedy forward selection + NNLS refit** (long-only, sum-to-one)\n", "  2. **L1-regularized convex regression (cvxpy)** with long-only and budget constraint\n", "- Train/validation split and basic metrics: in-sample/out-of-sample tracking error, number of names\n", "\n", "You can later add:\n", "- **Cardinality-constrained MIQP**, **factor-based tracking**, **sector caps**, and **turnover penalties**.\n", "\n", "**Tip:** If Wikipedia scrape of S&P 500 tickers fails (network/policy), the code falls back to a small demonstration universe.\n"]}, {"cell_type": "code", "metadata": {"id": "install"}, "source": ["# If running in a fresh Colab session, uncomment the next line to install packages:\n", "# !pip -q install yfinance cvxpy pulp ortools scikit-learn pandas numpy matplotlib > /dev/null\n"]}, {"cell_type": "code", "metadata": {"id": "imports"}, "source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "from datetime import datetime, timedelta\n", "\n", "import yfinance as yf\n", "import cvxpy as cp\n", "\n", "np.set_printoptions(suppress=True, linewidth=120)\n"]}, {"cell_type": "code", "metadata": {"id": "config"}, "source": ["# ===== Config =====\n", "BENCHMARK = '^GSPC'   # S&P 500 index ticker in Yahoo Finance\n", "TRAIN_YEARS = 3        # years in train window\n", "TEST_YEARS = 1         # years in test window following train\n", "FREQ = '1d'            # '1d' daily; can change to '1wk'\n", "MAX_TICKERS = 500      # cap for universe size (S&P 500)\n", "SEED = 42\n", "np.random.seed(SEED)\n", "\n", "# Dates\n", "end_date = datetime.today().date()\n", "start_date = end_date - timedelta(days=int((TRAIN_YEARS+TEST_YEARS)*365.25))\n", "split_date = end_date - timedelta(days=int(TEST_YEARS*365.25))\n", "start_date, split_date, end_date\n"]}, {"cell_type": "code", "metadata": {"id": "tickers"}, "source": ["def get_sp500_tickers(max_n=500):\n", "    \"\"\"Try to fetch S&P 500 tickers from Wikipedia; fall back to a demo list if it fails.\"\"\"\n", "    try:\n", "        tables = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n", "        df = tables[0]\n", "        tix = df['Symbol'].astype(str).str.replace('.', '-', regex=False).tolist()\n", "        # Some tickers are weird (e.g., BRK.B -> BRK-B). Keep only alnum + '-'\n", "        tix = [t for t in tix if len(t) > 0 and set(t) <= set('ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-')]\n", "        return tix[:max_n]\n", "    except Exception as e:\n", "        print('Wiki tickers fetch failed, using a small demo universe:', e)\n", "        return ['AAPL','MSFT','NVDA','AMZN','GOOGL','META','BRK-B','JPM','XOM','UNH','JNJ','PG','AVGO','CVX','HD','LLY','ADBE','NFLX','MRK','PEP'][:max_n]\n", "\n", "tickers = get_sp500_tickers(MAX_TICKERS)\n", "len(tickers), tickers[:10]\n"]}, {"cell_type": "code", "metadata": {"id": "prices"}, "source": ["def download_prices(tickers, start, end, interval='1d'):\n", "    data = yf.download(tickers + [BENCHMARK], start=start, end=end, interval=interval, auto_adjust=True, progress=False)\n", "    px = data['Close']\n", "    # Drop tickers with all-NaN\n", "    px = px.dropna(axis=1, how='all')\n", "    # Forward-fill occasional gaps\n", "    px = px.fillna(method='ffill')\n", "    return px\n", "\n", "prices = download_prices(tickers, start_date, end_date, FREQ)\n", "prices.tail(3)\n"]}, {"cell_type": "code", "metadata": {"id": "returns"}, "source": ["# Compute simple returns\n", "rets = prices.pct_change().dropna(how='all')\n", "rets = rets.loc[rets.index >= pd.to_datetime(start_date)]\n", "rets = rets.dropna(axis=1, how='any')  # keep only full-history names for simplicity\n", "\n", "# Separate benchmark and stock matrix\n", "r_b = rets[BENCHMARK]\n", "R = rets.drop(columns=[BENCHMARK])\n", "\n", "print('Shapes -> R:', R.shape, 'r_b:', r_b.shape)\n", "R.head(3)\n"]}, {"cell_type": "code", "metadata": {"id": "split"}, "source": ["# Train/validation split (time-based)\n", "train_mask = R.index < pd.to_datetime(split_date)\n", "R_tr, R_te = R.loc[train_mask], R.loc[~train_mask]\n", "rb_tr, rb_te = r_b.loc[train_mask], r_b.loc[~train_mask]\n", "R_tr.shape, R_te.shape, rb_tr.shape, rb_te.shape\n"]}, {"cell_type": "code", "metadata": {"id": "helpers"}, "source": ["def nnls_sum_to_one(R, r, idx=None):\n", "    \"\"\"Solve min ||R w - r||^2 s.t. w>=0, 1'w=1. If idx provided, restrict to those columns.\"\"\"\n", "    if idx is not None:\n", "        R_ = R.iloc[:, idx].values\n", "    else:\n", "        R_ = R.values\n", "    r_ = r.values.reshape(-1,1)\n", "    n = R_.shape[1]\n", "    w = cp.Variable((n,1))\n", "    obj = cp.Minimize(cp.sum_squares(R_ @ w - r_))\n", "    cons = [w >= 0, cp.sum(w) == 1]\n", "    prob = cp.Problem(obj, cons)\n", "    prob.solve(solver=cp.OSQP, verbose=False)\n", "    if w.value is None:\n", "        return None, np.inf\n", "    wv = w.value.flatten()\n", "    resid = (R_ @ w.value - r_).flatten()\n", "    te = np.std(resid) * np.sqrt(252)  # daily -> annualized TE\n", "    return wv, te\n", "\n", "def greedy_forward_nnls(R, r, k=15):\n", "    \"\"\"Greedy selection by correlation, with NNLS refit (sum-to-one) at each step.\"\"\"\n", "    remaining = list(range(R.shape[1]))\n", "    chosen = []\n", "    w = None\n", "    resid = r.values.copy()\n", "    for step in range(k):\n", "        # Pick the column with highest |corr| to current residual\n", "        corr = np.abs(np.corrcoef(R.values.T, resid)[R.shape[1]:, :R.shape[1]].diagonal()) if False else None\n", "        # Simpler & robust: brute-force best next feature by NNLS fit improvement\n", "        best_i, best_te = None, np.inf\n", "        for i in remaining:\n", "            idx = chosen + [i]\n", "            w_try, te_try = nnls_sum_to_one(R, r, idx)\n", "            if te_try < best_te:\n", "                best_te = te_try\n", "                best_i = i\n", "        chosen.append(best_i)\n", "        remaining.remove(best_i)\n", "        w, te = nnls_sum_to_one(R, r, chosen)\n", "        # Update residual for logging (not used for selection to keep it simple)\n", "        resid = r.values - R.iloc[:, chosen].values @ w.reshape(-1,1)\n", "        # print(f\"Step {step+1}: added {R.columns[best_i]}, TE~{te:.2f} bp\")\n", "    return chosen, w, te\n", "\n", "def l1_cvx(R, r, lam=1.0):\n", "    \"\"\"Solve min ||R w - r||^2 + lam * ||w||_1 s.t. w>=0, 1'w=1.\"\"\"\n", "    R_ = R.values\n", "    r_ = r.values.reshape(-1,1)\n", "    n = R_.shape[1]\n", "    w = cp.Variable((n,1))\n", "    obj = cp.Minimize(cp.sum_squares(R_ @ w - r_) + lam * cp.norm1(w))\n", "    cons = [w >= 0, cp.sum(w) == 1]\n", "    prob = cp.Problem(obj, cons)\n", "    prob.solve(solver=cp.OSQP, verbose=False)\n", "    if w.value is None:\n", "        return None, np.inf\n", "    wv = w.value.flatten()\n", "    resid = (R_ @ w.value - r_).flatten()\n", "    te = np.std(resid) * np.sqrt(252)\n", "    return wv, te\n", "\n", "def evaluate_te(R, r, w):\n", "    resid = r.values - R.values @ w\n", "    return float(np.std(resid) * np.sqrt(252))\n", "\n", "def portfolio_returns(R, w):\n", "    return pd.Series(R.values @ w, index=R.index)\n"]}, {"cell_type": "code", "metadata": {"id": "fit_greedy"}, "source": ["# ===== Approach A: Greedy forward + NNLS on TRAIN, evaluate on TEST =====\n", "K = 15\n", "chosen, w_tr, te_tr = greedy_forward_nnls(R_tr, rb_tr, k=K)\n", "names = R_tr.columns[chosen]\n", "print(f\"Chosen ({len(chosen)}):\", list(names))\n", "print(f\"In-sample TE (annualized, bp): {te_tr:.2f}\")\n", "\n", "# Evaluate on test\n", "w_full = np.zeros(R_tr.shape[1])\n", "w_full[chosen] = w_tr\n", "te_te = evaluate_te(R_te.iloc[:, :], rb_te, w_full)\n", "print(f\"Out-of-sample TE (annualized, bp): {te_te:.2f}\")\n", "\n", "# Build test period cumulative returns vs benchmark\n", "port_te = portfolio_returns(R_te.iloc[:, :], w_full)\n", "cum_port = (1 + port_te).cumprod()\n", "cum_bench = (1 + rb_te).cumprod()\n", "ax = (cum_port.rename('Portfolio') / cum_port.iloc[0]).plot(figsize=(9,4))\n", "((cum_bench.rename('Benchmark') / cum_bench.iloc[0])).plot(ax=ax)\n", "plt.title('Out-of-sample cumulative performance (Greedy+NNLS)')\n", "plt.legend(); plt.grid(True); plt.show()\n"]}, {"cell_type": "code", "metadata": {"id": "fit_l1"}, "source": ["# ===== Approach B: L1-regularized convex regression (with constraints) =====\n", "lams = np.geomspace(10.0, 0.01, 10)\n", "best = (None, None, np.inf)\n", "for lam in lams:\n", "    w_l1, te_in = l1_cvx(R_tr, rb_tr, lam=float(lam))\n", "    if w_l1 is None:\n", "        continue\n", "    te_out = evaluate_te(R_te, rb_te, w_l1)\n", "    nnz = int((w_l1 > 1e-6).sum())\n", "    # Simple score: prioritize low out-of-sample TE, break ties with sparsity\n", "    score = te_out + 0.01 * nnz\n", "    print(f\"lam={lam:.3f} -> OOS TE={te_out:.2f} bp, nnz={nnz}\")\n", "    if score < best[2]:\n", "        best = (w_l1, lam, score)\n", "\n", "w_best, lam_best, _ = best\n", "if w_best is not None:\n", "    print(f\"\\nBest lam={lam_best:.4f}; nnz={int((w_best>1e-6).sum())}; OOS TE={evaluate_te(R_te, rb_te, w_best):.2f} bp\")\n", "    port_te2 = portfolio_returns(R_te, w_best)\n", "    cum_port2 = (1 + port_te2).cumprod()\n", "    cum_bench2 = (1 + rb_te).cumprod()\n", "    ax = (cum_port2.rename('Portfolio L1') / cum_port2.iloc[0]).plot(figsize=(9,4))\n", "    ((cum_bench2.rename('Benchmark') / cum_bench2.iloc[0])).plot(ax=ax)\n", "    plt.title('Out-of-sample cumulative performance (L1 constrained)')\n", "    plt.legend(); plt.grid(True); plt.show()\n", "else:\n", "    print('L1 solver failed to find a solution. Try adjusting the lambda grid or OSQP install.')\n"]}, {"cell_type": "markdown", "metadata": {"id": "next-steps"}, "source": ["## Next steps\n", "- Add **sector caps**: restrict weights by sector buckets.\n", "- Add **turnover penalty**: fit sequentially over time with \\(\\|w_t - w_{t-1}\\|_1\\) penalty.\n", "- Try **MIQP** with cardinality (\\(\\sum z_i \\le k\\)). Use `pulp` or `ortools`.\n", "- Build **factor exposures** (PCA or Fama\u2013French-style) and solve a smaller problem matching exposures.\n", "- Replace benchmark returns with an **equal-weight** or **custom synthetic** benchmark to avoid mega-cap dominance.\n"]}]}